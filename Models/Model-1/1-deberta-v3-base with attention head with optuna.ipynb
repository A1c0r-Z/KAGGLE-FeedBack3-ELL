{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159a2f6f-57ed-4615-8a3e-6ab5f4b42ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    wandb=True\n",
    "    competition='FB3'\n",
    "    _wandb_kernel='nakama'\n",
    "    debug=False\n",
    "    apex=True\n",
    "    print_freq=20\n",
    "    num_workers=4\n",
    "    model=\"microsoft/deberta-v3-base\"\n",
    "    gradient_checkpointing=True\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=8\n",
    "    epochs=3\n",
    "    encoder_lr=5e-5\n",
    "    decoder_lr=1e-4\n",
    "    min_lr=1e-7\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=1\n",
    "    max_len=None\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    seed=42\n",
    "    n_fold=4\n",
    "    trn_fold=[0, 1, 2, 3]\n",
    "    train=True\n",
    "    llrd = 0.97\n",
    "    loss_weight = [0.21,0.16,0.10, 0.16, 0.21,0.16]\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4712d1fd-8b76-4e58-a479-6cca0145993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '/home/u210810417/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dbfd551-b331-42c9-b289-9c081d2f820d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \n",
      "Get your W&B access token from here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manony-mouse-430222\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/u210810417/jupyterlab/wandb/run-20221214_014818-1avw6a02</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/anony-mouse-430222/FB3-Public/runs/1avw6a02?apiKey=c431c51ede64e538dfa49f98174300bdce546d53\" target=\"_blank\">microsoft/deberta-v3-base</a></strong> to <a href=\"https://wandb.ai/anony-mouse-430222/FB3-Public?apiKey=c431c51ede64e538dfa49f98174300bdce546d53\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb\n",
    "# ====================================================\n",
    "if CFG.wandb:\n",
    "    \n",
    "    import wandb\n",
    "\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "        wandb.login(key=secret_value_0)\n",
    "        anony = None\n",
    "    except:\n",
    "        anony = \"must\"\n",
    "        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
    "\n",
    "\n",
    "    def class2dict(f):\n",
    "        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "    run = wandb.init(project='FB3-Public', \n",
    "                     name=CFG.model,\n",
    "                     config=class2dict(CFG),\n",
    "                     group=CFG.model,\n",
    "                     job_type=\"train\",\n",
    "                     anonymous=anony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ba05c6-0505-4e19-9844-c7c146bb21d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: iterative-stratification==0.1.7 in /home/u210810417/.local/lib/python3.7/site-packages (0.1.7)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.0.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.17.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.7) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.7) (1.2.0)\n",
      "tokenizers.__version__: 0.12.1\n",
      "transformers.__version__: 4.21.2\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "os.system('pip install iterative-stratification==0.1.7')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# os.system('pip uninstall -y transformers')\n",
    "# os.system('pip uninstall -y tokenizers')\n",
    "# os.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels transformers')\n",
    "# os.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels tokenizers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13f62b76-dbe7-4d1d-8036-0d130c8ee872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:,i]\n",
    "        y_pred = y_preds[:,i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb70c3f-597f-4c9f-8b36-4aae62c6b891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (3911, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5     3.5         3.0          3.0      4.0          3.0\n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5     2.5         3.0          2.0      2.0          2.5\n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0     3.5         3.0          3.0      3.0          2.5\n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5     4.5         4.5          4.5      4.0          5.0\n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5     3.0         3.0          3.0      2.5          2.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.shape: (3, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>when a person has no experience on a job their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>Do you think students would benefit from being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>Thomas Jefferson once states that \"it is wonde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text\n",
       "0  0000C359D63E  when a person has no experience on a job their...\n",
       "1  000BAD50D026  Do you think students would benefit from being...\n",
       "2  00367BB2546B  Thomas Jefferson once states that \"it is wonde..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.shape: (3, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n",
       "0  0000C359D63E       3.0     3.0         3.0          3.0      3.0          3.0\n",
       "1  000BAD50D026       3.0     3.0         3.0          3.0      3.0          3.0\n",
       "2  00367BB2546B       3.0     3.0         3.0          3.0      3.0          3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv('/home/u210810417/train.csv')\n",
    "test = pd.read_csv('/home/u210810417/test.csv')\n",
    "submission = pd.read_csv('/home/u210810417/sample_submission.csv')\n",
    "\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "display(train.head())\n",
    "print(f\"test.shape: {test.shape}\")\n",
    "display(test.head())\n",
    "print(f\"submission.shape: {submission.shape}\")\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac82cd9-742a-423d-848c-dc9517f104c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    978\n",
       "1    977\n",
       "2    978\n",
       "3    978\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols])):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2218a95c-3328-413e-8788-175b461fe474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (TransientError), entering retry loop.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# tokenizer\n",
    "# ====================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c570a45-eee2-4bd8-9b6a-d7f9b8046d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987cac73ee5c4f1180a3167052e3bffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3911 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_len: 1429\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "lengths = []\n",
    "tk0 = tqdm(train['full_text'].fillna(\"\").values, total=len(train))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "CFG.max_len = max(lengths) + 3 # cls & sep & sep\n",
    "# CFG.max_len = 512\n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3ac9d07-5155-4cb3-b2d4-ddc47c2ddf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        max_length=CFG.max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['full_text'].values\n",
    "        self.labels = df[cfg.target_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        return inputs, label\n",
    "    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba38450-f2cb-47bd-b84c-7f05bb004db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "    \n",
    "# class WeightedLayerPooling(nn.Module):\n",
    "#     def __init__(self, num_hidden_layers, layer_start: int = 9, layer_weights = None):\n",
    "#         super(WeightedLayerPooling, self).__init__()\n",
    "#         self.layer_start = layer_start\n",
    "#         self.num_hidden_layers = num_hidden_layers\n",
    "#         self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "#             else nn.Parameter(\n",
    "#                 torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
    "#             )\n",
    "\n",
    "#     def forward(self, features):\n",
    "\n",
    "#         all_layer_embedding = torch.stack(features)\n",
    "#         all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n",
    "\n",
    "#         weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "#         weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "\n",
    "#         return weighted_average\n",
    "    \n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "            LOGGER.info(self.config)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "            self.config.output_hidden_layers = True\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "            self.config.output_hidden_layers = True\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "#         self.config.update({'output_hidden_states':True})\n",
    "        # self.pool = WeightedLayerPooling(self.config.num_hidden_layers)\n",
    "        self.pool = MeanPooling()\n",
    "        \n",
    "        in_dim = self.config.hidden_size\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, 1),\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for module in self.attention.modules():\n",
    "            self._init_weights(module)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        REINIT_LAYERS = 1\n",
    "        for layer in self.model.encoder.layer[-REINIT_LAYERS:]:\n",
    "            for module in layer.modules():\n",
    "                self._init_weights(module)\n",
    "            print('Done')\n",
    "     \n",
    "            \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight = torch.nn.init.kaiming_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "##############################################\n",
    "#     def forward(self, inputs):\n",
    "# #         self.config.update({'output_hidden_states':True})\n",
    "#         outputs = self.model(**inputs)\n",
    "        \n",
    "        \n",
    "#         feature = self.pool(outputs.hidden_states)\n",
    "\n",
    "#         return self.fc(feature)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "#     def feature(self, inputs):\n",
    "#         outputs = self.model(**inputs)\n",
    "#         last_hidden_states = outputs[0]\n",
    "#         feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "#         return feature\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         feature = self.feature(inputs)\n",
    "#         output = self.fc(feature)\n",
    "#         return output\n",
    "\n",
    "##########################################################\n",
    "    def forward(self,inputs):\n",
    "\n",
    "        outputs = self.model(**inputs)\n",
    "        x = outputs[0]\n",
    "        mask = inputs['attention_mask']\n",
    "        w = self.attention(x).float()\n",
    "        w[mask==0]=float('-inf')\n",
    "        w = torch.softmax(w,1)\n",
    "        x = torch.sum(w * x, dim=1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f942dde-514c-49c3-b881-20725f1b7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Loss\n",
    "# ====================================================\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, reduction='mean', eps=1e-9):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b148269-0b6d-4e66-8df4-d0044d7b37ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighted_Loss(nn.Module):\n",
    "    def __init__(self,criterion):\n",
    "        super().__init__()\n",
    "        self.criterion = criterion\n",
    "    def forward(self,y_pred,y_true):\n",
    "        loss = 0\n",
    "        for i in range(len(CFG.target_cols)):\n",
    "            loss += self.criterion(y_pred[:,i],y_true[:,i])*CFG.loss_weight[i]\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59f6a716-3a08-4b83-96a1-67b39a5935f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d241a61-1371-4920-983d-7d61140da53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.9):\n",
    "        \n",
    "        \n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        layer_names = []\n",
    "\n",
    "        for idx, (name, param) in enumerate(model.model.named_parameters()):\n",
    "            if not any(nd in name for nd in no_decay):\n",
    "                layer_names.append(name)\n",
    "            # print(f'{idx}: {name}')\n",
    "            \n",
    "        layer_names.reverse()\n",
    "        lr      = encoder_lr\n",
    "        lr_mult = CFG.llrd\n",
    "        #0.995 0.4444\n",
    "        # print(layer_names)\n",
    "        # placeholder\n",
    "        parameters = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        # store params & learning rates\n",
    "        for idx, name in enumerate(layer_names):\n",
    "    # display info\n",
    "            # print(f'{idx}: lr = {lr:.6f}, {name}')\n",
    "    \n",
    "    # append layer parameters\n",
    "            parameters += [{'params': [p for n, p in model.model.named_parameters() if n == name],\n",
    "                    'lr':lr, 'weight_decay':weight_decay}]\n",
    "    \n",
    "    # update learning rate\n",
    "            lr *= lr_mult\n",
    "        parameters +=[{'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}]\n",
    "        for n,p in model.named_parameters():\n",
    "            if \"model\" not in n:\n",
    "                if \"attention\" in n:\n",
    "                    print(n)\n",
    "        # print(parameters)\n",
    "        # print(parameters)\n",
    "        return parameters\n",
    "\n",
    "#     def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.9):\n",
    "        \n",
    "        \n",
    "#         no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "#         layer_names = []\n",
    "\n",
    "#         for idx, (name, param) in enumerate(model.model.named_parameters()):\n",
    "#             if not any(nd in name for nd in no_decay):\n",
    "#                 layer_names.append(name)\n",
    "#             # print(f'{idx}: {name}')\n",
    "            \n",
    "#         layer_names.reverse()\n",
    "#         lr      = encoder_lr\n",
    "#         lr_mult = 0.993\n",
    "#         #0.995 0.4444\n",
    "#         # print(layer_names)\n",
    "#         # placeholder\n",
    "#         parameters = []\n",
    "        \n",
    "        \n",
    "        \n",
    "#         # store params & learning rates\n",
    "#         for idx, name in enumerate(layer_names):\n",
    "#     # display info\n",
    "#             # print(f'{idx}: lr = {lr:.6f}, {name}')\n",
    "    \n",
    "#     # append layer parameters\n",
    "#             parameters += [{'params': [p for n, p in model.model.named_parameters() if n == name],\n",
    "#                     'lr':lr, 'weight_decay':weight_decay}]\n",
    "    \n",
    "#     # update learning rate\n",
    "#             lr *= lr_mult\n",
    "#         parameters +=[{'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#              'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "#             {'params': [p for n, p in model.named_parameters() if \"model\" not in n and \"attention\" in n],\n",
    "#              'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "#             {'params': [p for n, p in model.named_parameters() if \"model\" not in n and \"attention\" not in n],\n",
    "#              'lr': decoder_lr, 'weight_decay': 0.0}]\n",
    "#         for n,p in model.named_parameters():\n",
    "#             if \"model\" not in n:\n",
    "#                 if \"attention\" in n:\n",
    "#                     print(n)\n",
    "#         # print(parameters)\n",
    "#         # print(parameters)\n",
    "#         return parameters\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = Weighted_Loss(nn.SmoothL1Loss(reduction='mean')) # RMSELoss(reduction=\"mean\")\n",
    " \n",
    "    best_score = np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        \n",
    "        # scoring\n",
    "        score, scores = get_score(valid_labels, predictions)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "\n",
    "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5ee9efa-165d-43c9-a51c-739c266b7b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     def get_result(oof_df):\n",
    "#         labels = oof_df[CFG.target_cols].values\n",
    "#         preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
    "#         score, scores = get_score(labels, preds)\n",
    "#         LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
    "    \n",
    "#     if CFG.train:\n",
    "#         oof_df = pd.DataFrame()\n",
    "#         for fold in range(CFG.n_fold):\n",
    "#             if fold in CFG.trn_fold:\n",
    "#                 _oof_df = train_loop(train, fold)\n",
    "#                 oof_df = pd.concat([oof_df, _oof_df])\n",
    "#                 LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "#                 get_result(_oof_df)\n",
    "#         oof_df = oof_df.reset_index(drop=True)\n",
    "#         LOGGER.info(f\"========== CV ==========\")\n",
    "#         get_result(oof_df)\n",
    "#         oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
    "        \n",
    "#     if CFG.wandb:\n",
    "#         wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f46db43-d5ce-4010-9b4a-de78bdbaf51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-14 01:48:52,156]\u001b[0m A new study created in memory with name: no-name-7f767321-9887-4d3b-8957-db8e28813799\u001b[0m\n",
      "========== fold: 0 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 1s (remain 88m 37s) Loss: 2.1010(2.1010) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 6m 50s (remain 0m 0s) Loss: 0.0917(0.1762) Grad: 186656.6875  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 44s) Loss: 0.0836(0.0836) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1762  avg_val_loss: 0.1180  time: 450s\n",
      "Epoch 1 - Score: 0.4783  Scores: [0.5388780363028517, 0.4407438434289395, 0.41600740238790335, 0.4945145057247125, 0.500524024628813, 0.4792564375476278]\n",
      "Epoch 1 - Save Best Score: 0.4783 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0891(0.1180) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 18m 42s) Loss: 0.0764(0.0764) Grad: 360640.2500  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 6m 50s (remain 0m 0s) Loss: 0.4474(0.1145) Grad: 406937.9375  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 35s) Loss: 0.0994(0.0994) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1145  avg_val_loss: 0.1094  time: 450s\n",
      "Epoch 2 - Score: 0.4607  Scores: [0.48272728814153354, 0.44295246831876356, 0.4121399877199979, 0.4548005388909946, 0.5205580824786445, 0.4509041387020568]\n",
      "Epoch 2 - Save Best Score: 0.4607 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 39s (remain 0m 0s) Loss: 0.1718(0.1094) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 19m 40s) Loss: 0.1107(0.1107) Grad: 397374.0938  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 6m 49s (remain 0m 0s) Loss: 0.0237(0.0890) Grad: 91700.8984  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 31s) Loss: 0.0913(0.0913) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0890  avg_val_loss: 0.1030  time: 450s\n",
      "Epoch 3 - Score: 0.4495  Scores: [0.4811280288916606, 0.4441163215575171, 0.4110403757782563, 0.4533870881421869, 0.4688590498454551, 0.43830030843951695]\n",
      "Epoch 3 - Save Best Score: 0.4495 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 39s (remain 0m 0s) Loss: 0.1122(0.1030) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2934] Elapsed 0m 0s (remain 18m 5s) Loss: 4.3462(4.3462) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2933/2934] Elapsed 6m 48s (remain 0m 0s) Loss: 0.0462(0.1807) Grad: 282606.0625  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 18s) Loss: 0.1055(0.1055) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1807  avg_val_loss: 0.1234  time: 448s\n",
      "Epoch 1 - Score: 0.4968  Scores: [0.51673493800886, 0.49730093920826296, 0.47394026247299686, 0.5727011567283309, 0.4694405477912625, 0.4509633824699739]\n",
      "Epoch 1 - Save Best Score: 0.4968 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0971(0.1234) \n",
      "Epoch: [2][0/2934] Elapsed 0m 0s (remain 19m 58s) Loss: 0.1161(0.1161) Grad: 408108.3125  LR: 0.00003753  \n",
      "Epoch: [2][2933/2934] Elapsed 6m 49s (remain 0m 0s) Loss: 0.1033(0.1155) Grad: 420775.7500  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 24s) Loss: 0.0567(0.0567) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1155  avg_val_loss: 0.1117  time: 449s\n",
      "Epoch 2 - Score: 0.4703  Scores: [0.4890629472804949, 0.4622692624942983, 0.4379964911910309, 0.4821853611819679, 0.48392889906933284, 0.4662387538648796]\n",
      "Epoch 2 - Save Best Score: 0.4703 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0419(0.1117) \n",
      "Epoch: [3][0/2934] Elapsed 0m 0s (remain 21m 36s) Loss: 0.0357(0.0357) Grad: 259511.8125  LR: 0.00001251  \n",
      "Epoch: [3][2933/2934] Elapsed 6m 48s (remain 0m 0s) Loss: 0.1169(0.0932) Grad: 465966.0000  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 24s) Loss: 0.0863(0.0863) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0932  avg_val_loss: 0.1042  time: 449s\n",
      "Epoch 3 - Score: 0.4527  Scores: [0.48920672879097804, 0.44195898061803535, 0.41820679143375455, 0.4503278038647289, 0.4668670196960002, 0.4496337719271793]\n",
      "Epoch 3 - Save Best Score: 0.4527 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0425(0.1042) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 18m 50s) Loss: 2.3520(2.3520) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 6m 50s (remain 0m 0s) Loss: 0.0111(0.1776) Grad: 65440.8477  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 33s) Loss: 0.2617(0.2617) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1776  avg_val_loss: 0.1158  time: 447s\n",
      "Epoch 1 - Score: 0.4791  Scores: [0.49814686945509584, 0.4610343148919261, 0.4409630091356523, 0.5270578848672223, 0.48544386761629854, 0.46165991469839784]\n",
      "Epoch 1 - Save Best Score: 0.4791 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 36s (remain 0m 0s) Loss: 0.1011(0.1158) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 24s) Loss: 0.1494(0.1494) Grad: 489782.4688  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 6m 52s (remain 0m 0s) Loss: 0.1102(0.1141) Grad: 198802.5000  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 35s) Loss: 0.1867(0.1867) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1141  avg_val_loss: 0.1123  time: 449s\n",
      "Epoch 2 - Score: 0.4701  Scores: [0.5001465287388327, 0.4519217289503702, 0.4249288970994163, 0.4762567705072353, 0.4990014464985306, 0.4684161287185546]\n",
      "Epoch 2 - Save Best Score: 0.4701 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0594(0.1123) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 19m 35s) Loss: 0.1429(0.1429) Grad: inf  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 6m 52s (remain 0m 0s) Loss: 0.0598(0.0886) Grad: 153859.1406  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 32s) Loss: 0.1907(0.1907) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0886  avg_val_loss: 0.1073  time: 449s\n",
      "Epoch 3 - Score: 0.4598  Scores: [0.4808619067652019, 0.4509463579061564, 0.4174169582469625, 0.466814477951571, 0.4854742354098303, 0.45725063395989335]\n",
      "Epoch 3 - Save Best Score: 0.4598 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0629(0.1073) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 20m 46s) Loss: 1.8758(1.8758) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 6m 38s (remain 0m 0s) Loss: 0.3153(0.1814) Grad: 387102.5938  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 4s) Loss: 0.2127(0.2127) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1814  avg_val_loss: 0.1290  time: 424s\n",
      "Epoch 1 - Score: 0.5045  Scores: [0.5567756302532237, 0.48504074754136134, 0.46872509489333924, 0.4693187533023757, 0.5201707239366486, 0.5271573538909787]\n",
      "Epoch 1 - Save Best Score: 0.5045 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0439(0.1290) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 21m 6s) Loss: 0.0690(0.0690) Grad: 348813.2188  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 25s (remain 0m 0s) Loss: 0.1278(0.1206) Grad: 242910.6719  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 5s) Loss: 0.2326(0.2326) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1206  avg_val_loss: 0.1049  time: 352s\n",
      "Epoch 2 - Score: 0.4552  Scores: [0.4972786798414288, 0.45074148783444984, 0.43478927350799174, 0.4403821987453609, 0.4630074298559645, 0.4449899899558672]\n",
      "Epoch 2 - Save Best Score: 0.4552 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0486(0.1049) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 22m 13s) Loss: 0.0501(0.0501) Grad: 278780.5000  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 25s (remain 0m 0s) Loss: 0.0384(0.0903) Grad: 115499.2969  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 8s) Loss: 0.1695(0.1695) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0903  avg_val_loss: 0.1007  time: 351s\n",
      "Epoch 3 - Score: 0.4446  Scores: [0.48021089679527984, 0.4456582386875365, 0.40738386498453666, 0.43613685021756016, 0.46453912332623104, 0.43386311613546075]\n",
      "Epoch 3 - Save Best Score: 0.4446 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0477(0.1007) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== CV ==========\n",
      "Score: 0.4517  Scores: [0.48286431284891274, 0.44568327073251157, 0.41353526708610205, 0.45179847502684845, 0.4715081935079777, 0.44485634353796044]\n",
      "\u001b[32m[I 2022-12-14 03:15:35,099]\u001b[0m Trial 0 finished with value: 0.4517076437900522 and parameters: {'epoch': 4, 'encoder_lr': 0.00037409434091301814, 'llrd': 0.8885960944661698}. Best is trial 0 with value: 0.4517076437900522.\u001b[0m\n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111111111111111111111111111111\n",
      "0.4517076437900522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 20m 57s) Loss: 2.0664(2.0664) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.1119(0.1743) Grad: 451114.3750  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 42s) Loss: 0.1616(0.1616) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1743  avg_val_loss: 0.1428  time: 350s\n",
      "Epoch 1 - Score: 0.5259  Scores: [0.5172925495775443, 0.47049240179294216, 0.4487275051595862, 0.49131722155170793, 0.595346902213994, 0.6321915305155291]\n",
      "Epoch 1 - Save Best Score: 0.5259 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.2762(0.1428) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 18m 48s) Loss: 0.0785(0.0785) Grad: 375796.0625  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.0910(0.1151) Grad: 407038.4062  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 39s) Loss: 0.0898(0.0898) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1151  avg_val_loss: 0.1051  time: 351s\n",
      "Epoch 2 - Score: 0.4531  Scores: [0.4955007513478271, 0.43976369580647107, 0.4129197366178905, 0.4552416186147978, 0.47949940829845983, 0.4355701009638151]\n",
      "Epoch 2 - Save Best Score: 0.4531 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 26s (remain 0m 0s) Loss: 0.1060(0.1051) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 20m 9s) Loss: 0.0460(0.0460) Grad: 289529.2188  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 24s (remain 0m 0s) Loss: 0.1316(0.0934) Grad: 462550.5000  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 36s) Loss: 0.0771(0.0771) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0934  avg_val_loss: 0.1018  time: 352s\n",
      "Epoch 3 - Score: 0.4468  Scores: [0.4805009643716676, 0.4399999697808841, 0.40669062342351975, 0.4529585995068709, 0.46345221081353577, 0.4369733074774361]\n",
      "Epoch 3 - Save Best Score: 0.4468 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1276(0.1018) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2934] Elapsed 0m 0s (remain 18m 55s) Loss: 3.0567(3.0567) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2933/2934] Elapsed 5m 22s (remain 0m 0s) Loss: 0.0699(0.1755) Grad: 328987.5938  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 38s) Loss: 0.1386(0.1386) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1755  avg_val_loss: 0.1432  time: 350s\n",
      "Epoch 1 - Score: 0.5433  Scores: [0.4957198099544014, 0.4612282632733822, 0.6720436480939915, 0.4593341229582834, 0.5527103576477268, 0.6190037468036172]\n",
      "Epoch 1 - Save Best Score: 0.5433 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0695(0.1432) \n",
      "Epoch: [2][0/2934] Elapsed 0m 0s (remain 19m 1s) Loss: 0.1108(0.1108) Grad: 399584.8750  LR: 0.00003753  \n",
      "Epoch: [2][2933/2934] Elapsed 5m 24s (remain 0m 0s) Loss: 0.0444(0.1191) Grad: 142369.1094  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 34s) Loss: 0.0797(0.0797) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1191  avg_val_loss: 0.1312  time: 352s\n",
      "Epoch 2 - Score: 0.5124  Scores: [0.497307178167604, 0.4916110052881887, 0.4975347021360474, 0.6019615586812828, 0.5259817647936514, 0.45979498987307643]\n",
      "Epoch 2 - Save Best Score: 0.5124 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0425(0.1312) \n",
      "Epoch: [3][0/2934] Elapsed 0m 0s (remain 19m 27s) Loss: 0.0858(0.0858) Grad: 355359.9688  LR: 0.00001251  \n",
      "Epoch: [3][2933/2934] Elapsed 5m 22s (remain 0m 0s) Loss: 0.0713(0.0927) Grad: 162784.6250  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 42s) Loss: 0.0904(0.0904) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0927  avg_val_loss: 0.1050  time: 350s\n",
      "Epoch 3 - Score: 0.4546  Scores: [0.49168866694566293, 0.44196119899718145, 0.41956487157083794, 0.45440562527932804, 0.470203344308001, 0.4498101290624786]\n",
      "Epoch 3 - Save Best Score: 0.4546 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0402(0.1050) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 20m 5s) Loss: 2.7799(2.7799) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 25s (remain 0m 0s) Loss: 0.2223(0.1702) Grad: 580507.9375  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 31s) Loss: 0.1288(0.1288) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1702  avg_val_loss: 0.1311  time: 351s\n",
      "Epoch 1 - Score: 0.5097  Scores: [0.5109555439545256, 0.47272809682366224, 0.4574921959408412, 0.481233155189213, 0.47660170522866774, 0.6590328484021348]\n",
      "Epoch 1 - Save Best Score: 0.5097 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0401(0.1311) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 7s) Loss: 0.0332(0.0332) Grad: 217874.4688  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 26s (remain 0m 0s) Loss: 0.1593(0.1182) Grad: 269678.1875  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 31s) Loss: 0.1713(0.1713) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1182  avg_val_loss: 0.1110  time: 364s\n",
      "Epoch 2 - Score: 0.4689  Scores: [0.4891768190208495, 0.45054446453718144, 0.4437568932629798, 0.46721342974541996, 0.4994608659931428, 0.463352034229049]\n",
      "Epoch 2 - Save Best Score: 0.4689 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0813(0.1110) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 20m 38s) Loss: 0.1419(0.1419) Grad: 449589.7500  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 7m 44s (remain 0m 0s) Loss: 0.0438(0.0919) Grad: 134821.9375  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 41s) Loss: 0.1816(0.1816) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0919  avg_val_loss: 0.1065  time: 509s\n",
      "Epoch 3 - Score: 0.4579  Scores: [0.48269190272524765, 0.44931692231836956, 0.4160471995182135, 0.4668419567768865, 0.4790860560731076, 0.4536699476376514]\n",
      "Epoch 3 - Save Best Score: 0.4579 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 44s (remain 0m 0s) Loss: 0.0777(0.1065) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 20m 49s) Loss: 3.2280(3.2280) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 7m 41s (remain 0m 0s) Loss: 0.0376(0.1817) Grad: 134105.5625  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 44s) Loss: 0.2079(0.2079) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1817  avg_val_loss: 0.1097  time: 507s\n",
      "Epoch 1 - Score: 0.4663  Scores: [0.4949315323210375, 0.47055972059854423, 0.4556814492589336, 0.44731396152492153, 0.4928946813848514, 0.4363076387835738]\n",
      "Epoch 1 - Save Best Score: 0.4663 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 45s (remain 0m 0s) Loss: 0.0454(0.1097) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 21m 14s) Loss: 0.1199(0.1199) Grad: 484387.1562  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 7m 43s (remain 0m 0s) Loss: 0.1670(0.1195) Grad: 247606.5000  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 49s) Loss: 0.3308(0.3308) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1195  avg_val_loss: 0.1435  time: 505s\n",
      "Epoch 2 - Score: 0.5191  Scores: [0.69835379048965, 0.48968122714526024, 0.44022327360995844, 0.5547349926265721, 0.5017736566733302, 0.42971118541816095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 41s (remain 0m 0s) Loss: 0.0357(0.1435) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 18m 55s) Loss: 0.4311(0.4311) Grad: inf  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 7m 42s (remain 0m 0s) Loss: 0.0198(0.0916) Grad: 96150.9531  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 47s) Loss: 0.1836(0.1836) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0916  avg_val_loss: 0.1000  time: 509s\n",
      "Epoch 3 - Score: 0.4431  Scores: [0.4818257217273145, 0.44168492077621185, 0.4087597411186688, 0.43535810186097806, 0.46186833665873317, 0.4289999755199286]\n",
      "Epoch 3 - Save Best Score: 0.4431 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 46s (remain 0m 0s) Loss: 0.0523(0.1000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== CV ==========\n",
      "Score: 0.4507  Scores: [0.4841949358277174, 0.44325560130438735, 0.41279716051579396, 0.4525296440427369, 0.4687012652543952, 0.4424718999493954]\n",
      "\u001b[32m[I 2022-12-14 04:36:57,437]\u001b[0m Trial 1 finished with value: 0.45065841781573773 and parameters: {'epoch': 4, 'encoder_lr': 0.00014031439009558157, 'llrd': 0.9254840682752528}. Best is trial 1 with value: 0.45065841781573773.\u001b[0m\n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111111111111111111111111111111\n",
      "0.45065841781573773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 21m 25s) Loss: 4.1739(4.1739) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 7m 38s (remain 0m 0s) Loss: 0.0818(0.1812) Grad: 197642.6562  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 53s) Loss: 0.1062(0.1062) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1812  avg_val_loss: 0.1376  time: 507s\n",
      "Epoch 1 - Score: 0.5145  Scores: [0.6316107361598808, 0.4543700914886045, 0.44450277048438774, 0.499943073622846, 0.506262242328746, 0.5500731895468994]\n",
      "Epoch 1 - Save Best Score: 0.5145 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 48s (remain 0m 0s) Loss: 0.0887(0.1376) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 20m 13s) Loss: 0.1256(0.1256) Grad: inf  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 7m 41s (remain 0m 0s) Loss: 0.0763(0.1187) Grad: 168462.8750  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 8s) Loss: 0.0821(0.0821) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1187  avg_val_loss: 0.1115  time: 499s\n",
      "Epoch 2 - Score: 0.4674  Scores: [0.49911010456538035, 0.44367211738581913, 0.40998376320464125, 0.47839465772770545, 0.4679318098336716, 0.5054983083284335]\n",
      "Epoch 2 - Save Best Score: 0.4674 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1206(0.1115) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 18m 35s) Loss: 0.1409(0.1409) Grad: inf  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.0846(0.0913) Grad: 181711.1719  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 49s) Loss: 0.0862(0.0862) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0913  avg_val_loss: 0.1016  time: 351s\n",
      "Epoch 3 - Score: 0.4462  Scores: [0.4813091055936086, 0.4397801422960884, 0.4075128580989938, 0.4521408286597996, 0.4608050154336848, 0.43569665679661373]\n",
      "Epoch 3 - Save Best Score: 0.4462 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1421(0.1016) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2934] Elapsed 0m 0s (remain 19m 17s) Loss: 2.0190(2.0190) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2933/2934] Elapsed 5m 21s (remain 0m 0s) Loss: 0.0508(0.1792) Grad: 139156.0469  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 32s) Loss: 0.0881(0.0881) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1792  avg_val_loss: 0.1197  time: 349s\n",
      "Epoch 1 - Score: 0.4877  Scores: [0.5206245806652157, 0.4613757654463226, 0.49358772413769364, 0.4661502120924575, 0.5222108565809669, 0.4621443885139726]\n",
      "Epoch 1 - Save Best Score: 0.4877 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0733(0.1197) \n",
      "Epoch: [2][0/2934] Elapsed 0m 0s (remain 21m 11s) Loss: 0.1315(0.1315) Grad: 471248.0625  LR: 0.00003753  \n",
      "Epoch: [2][2933/2934] Elapsed 5m 23s (remain 0m 0s) Loss: 0.0440(0.1190) Grad: 132958.9219  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 50s) Loss: 0.0972(0.0972) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1190  avg_val_loss: 0.1078  time: 351s\n",
      "Epoch 2 - Score: 0.4605  Scores: [0.497307008042644, 0.452712209665756, 0.4202407761960833, 0.46675684566326464, 0.4745998575223091, 0.4512267033914086]\n",
      "Epoch 2 - Save Best Score: 0.4605 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0353(0.1078) \n",
      "Epoch: [3][0/2934] Elapsed 0m 0s (remain 19m 21s) Loss: 0.0423(0.0423) Grad: 265415.3125  LR: 0.00001251  \n",
      "Epoch: [3][2933/2934] Elapsed 5m 22s (remain 0m 0s) Loss: 0.0406(0.0907) Grad: 120515.5000  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 33s) Loss: 0.1011(0.1011) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0907  avg_val_loss: 0.1062  time: 350s\n",
      "Epoch 3 - Score: 0.4573  Scores: [0.49266607113804367, 0.4466852592145129, 0.42395670576942385, 0.456457081151788, 0.4731448337396238, 0.450846341325502]\n",
      "Epoch 3 - Save Best Score: 0.4573 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0380(0.1062) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 26m 29s) Loss: 2.7384(2.7384) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 26s (remain 0m 0s) Loss: 0.0420(0.1769) Grad: 272049.2812  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 35s) Loss: 0.2896(0.2896) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1769  avg_val_loss: 0.1319  time: 351s\n",
      "Epoch 1 - Score: 0.4988  Scores: [0.4914710867464422, 0.45950094778643763, 0.43241412539998564, 0.47933414780883077, 0.66796227513352, 0.4620266891637443]\n",
      "Epoch 1 - Save Best Score: 0.4988 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.1552(0.1319) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 36s) Loss: 0.0947(0.0947) Grad: 420111.5312  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 25s (remain 0m 0s) Loss: 0.0466(0.1141) Grad: 287442.1562  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 29s) Loss: 0.2508(0.2508) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1141  avg_val_loss: 0.1222  time: 350s\n",
      "Epoch 2 - Score: 0.4856  Scores: [0.5215671245345406, 0.4759792328680206, 0.42130945397283426, 0.4668892403548048, 0.5611007586730267, 0.4666329545121198]\n",
      "Epoch 2 - Save Best Score: 0.4856 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.1527(0.1222) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 20m 48s) Loss: 0.0677(0.0677) Grad: 352095.9375  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 25s (remain 0m 0s) Loss: 0.0530(0.0922) Grad: 291880.8438  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 31s) Loss: 0.1868(0.1868) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0922  avg_val_loss: 0.1067  time: 350s\n",
      "Epoch 3 - Score: 0.4580  Scores: [0.4841516254442488, 0.4472475798828593, 0.4161722717945147, 0.46620197402142594, 0.48163440323888534, 0.45252945238841824]\n",
      "Epoch 3 - Save Best Score: 0.4580 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0961(0.1067) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 22m 50s) Loss: 3.1727(3.1727) Grad: 1870995.2500  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 25s (remain 0m 0s) Loss: 0.0846(0.1746) Grad: 389210.7188  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 4m 12s) Loss: 0.1740(0.1740) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1746  avg_val_loss: 0.1287  time: 352s\n",
      "Epoch 1 - Score: 0.5144  Scores: [0.4941584628369391, 0.5079670381535866, 0.5947910400934159, 0.4488952684879778, 0.5160785218032955, 0.5244574614737169]\n",
      "Epoch 1 - Save Best Score: 0.5144 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0799(0.1287) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 15s) Loss: 0.0473(0.0473) Grad: 290081.5000  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 25s (remain 0m 0s) Loss: 0.1975(0.1174) Grad: 280602.5938  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 18s) Loss: 0.2156(0.2156) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1174  avg_val_loss: 0.1076  time: 351s\n",
      "Epoch 2 - Score: 0.4604  Scores: [0.4946432506672705, 0.4541273703806408, 0.4233987217220821, 0.4824466148102546, 0.4672034055557276, 0.4404842635755959]\n",
      "Epoch 2 - Save Best Score: 0.4604 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0723(0.1076) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 21m 30s) Loss: 0.0676(0.0676) Grad: 317312.8125  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 25s (remain 0m 0s) Loss: 0.0271(0.0927) Grad: 106851.3281  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 7s) Loss: 0.1600(0.1600) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0927  avg_val_loss: 0.1010  time: 351s\n",
      "Epoch 3 - Score: 0.4450  Scores: [0.48637049477379957, 0.4448585208362517, 0.40875979467288254, 0.4366560591331286, 0.46294545883204197, 0.43054588475526073]\n",
      "Epoch 3 - Save Best Score: 0.4450 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0524(0.1010) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== CV ==========\n",
      "Score: 0.4517  Scores: [0.4861406269172624, 0.4446520950777216, 0.4141501972861887, 0.45298840076109936, 0.46970580599745576, 0.4425039672363737]\n",
      "\u001b[32m[I 2022-12-14 05:52:43,922]\u001b[0m Trial 2 finished with value: 0.45169018221268353 and parameters: {'epoch': 3, 'encoder_lr': 0.00013261621572574755, 'llrd': 0.9270748439777742}. Best is trial 1 with value: 0.45065841781573773.\u001b[0m\n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111111111111111111111111111111\n",
      "0.45169018221268353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 20m 14s) Loss: 2.5176(2.5176) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.0457(0.1801) Grad: 129124.3750  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 49s) Loss: 0.0844(0.0844) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1801  avg_val_loss: 0.1303  time: 351s\n",
      "Epoch 1 - Score: 0.5146  Scores: [0.5213418737059692, 0.4425718603046448, 0.5557642143466208, 0.5964579743704863, 0.47113149106644614, 0.5006240229579185]\n",
      "Epoch 1 - Save Best Score: 0.5146 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0862(0.1303) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 20m 49s) Loss: 0.1059(0.1059) Grad: 399460.8750  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.0443(0.1188) Grad: 126736.8906  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 43s) Loss: 0.0943(0.0943) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1188  avg_val_loss: 0.1118  time: 351s\n",
      "Epoch 2 - Score: 0.4738  Scores: [0.48972304287255114, 0.4553688164307012, 0.4743417692991039, 0.48524294716577643, 0.46303980890348495, 0.4749794778930286]\n",
      "Epoch 2 - Save Best Score: 0.4738 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1469(0.1118) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 20m 20s) Loss: 0.2860(0.2860) Grad: 610537.1875  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 22s (remain 0m 0s) Loss: 0.0818(0.0893) Grad: 192452.8281  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 53s) Loss: 0.0730(0.0730) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0893  avg_val_loss: 0.1033  time: 350s\n",
      "Epoch 3 - Score: 0.4501  Scores: [0.48171529408386254, 0.4417385489470384, 0.41128254464154146, 0.45707429731114846, 0.4697309147726542, 0.438908930829927]\n",
      "Epoch 3 - Save Best Score: 0.4501 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1160(0.1033) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2934] Elapsed 0m 0s (remain 19m 32s) Loss: 3.2935(3.2935) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2933/2934] Elapsed 5m 21s (remain 0m 0s) Loss: 0.1181(0.1771) Grad: 221073.6250  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 39s) Loss: 0.1016(0.1016) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1771  avg_val_loss: 0.1146  time: 349s\n",
      "Epoch 1 - Score: 0.4789  Scores: [0.4956710619258705, 0.461315175082179, 0.47263159432412377, 0.46018089268239987, 0.48446179936629424, 0.49935280597519216]\n",
      "Epoch 1 - Save Best Score: 0.4789 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0439(0.1146) \n",
      "Epoch: [2][0/2934] Elapsed 0m 0s (remain 25m 27s) Loss: 0.1113(0.1113) Grad: inf  LR: 0.00003753  \n",
      "Epoch: [2][2933/2934] Elapsed 5m 23s (remain 0m 0s) Loss: 0.0363(0.1173) Grad: 107890.4219  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 44s) Loss: 0.1308(0.1308) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1173  avg_val_loss: 0.1246  time: 351s\n",
      "Epoch 2 - Score: 0.4994  Scores: [0.4886274353054846, 0.4915804894656028, 0.471786922118749, 0.45753173645931866, 0.48714134888948274, 0.5994638364938102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0301(0.1246) \n",
      "Epoch: [3][0/2934] Elapsed 0m 0s (remain 28m 54s) Loss: 0.0581(0.0581) Grad: 289128.5625  LR: 0.00001251  \n",
      "Epoch: [3][2933/2934] Elapsed 5m 22s (remain 0m 0s) Loss: 0.0578(0.0898) Grad: 147214.6562  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 44s) Loss: 0.0899(0.0899) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0898  avg_val_loss: 0.1051  time: 350s\n",
      "Epoch 3 - Score: 0.4546  Scores: [0.48872603236540746, 0.4436187062620155, 0.4202814515721467, 0.4534089412615355, 0.47473587964006453, 0.44655748562054065]\n",
      "Epoch 3 - Save Best Score: 0.4546 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0406(0.1051) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 20m 48s) Loss: 3.2353(3.2353) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 25s (remain 0m 0s) Loss: 0.1035(0.1752) Grad: 406972.9375  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 38s) Loss: 0.0816(0.0816) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1752  avg_val_loss: 0.1710  time: 350s\n",
      "Epoch 1 - Score: 0.5708  Scores: [0.7419943900887902, 0.6724515810643699, 0.45474003479852365, 0.5124870386518994, 0.4950098288521799, 0.5481956270669257]\n",
      "Epoch 1 - Save Best Score: 0.5708 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0441(0.1710) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 37s) Loss: 0.1691(0.1691) Grad: 530808.2500  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 25s (remain 0m 0s) Loss: 0.0973(0.1199) Grad: 204520.2031  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 44s) Loss: 0.1778(0.1778) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1199  avg_val_loss: 0.1113  time: 350s\n",
      "Epoch 2 - Score: 0.4675  Scores: [0.48623497886700007, 0.45567860351345874, 0.41862894557751623, 0.4806976149409129, 0.5039428796346231, 0.45962717173691714]\n",
      "Epoch 2 - Save Best Score: 0.4675 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0655(0.1113) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 20m 11s) Loss: 0.0252(0.0252) Grad: 205921.8906  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 25s (remain 0m 0s) Loss: 0.2670(0.0907) Grad: 342759.5938  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 44s) Loss: 0.2025(0.2025) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0907  avg_val_loss: 0.1072  time: 350s\n",
      "Epoch 3 - Score: 0.4593  Scores: [0.4797936051137622, 0.4485004755406109, 0.41867873905464853, 0.4650448450245268, 0.4892945420881495, 0.45440963631423475]\n",
      "Epoch 3 - Save Best Score: 0.4593 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0784(0.1072) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 21m 35s) Loss: 4.1007(4.1007) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 22s (remain 0m 0s) Loss: 0.1221(0.1760) Grad: 206247.1250  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 13s) Loss: 0.2541(0.2541) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1760  avg_val_loss: 0.1104  time: 348s\n",
      "Epoch 1 - Score: 0.4653  Scores: [0.5045555811532232, 0.4756047736535551, 0.4239720509843639, 0.4512159093235945, 0.4893893162712012, 0.44716471801038177]\n",
      "Epoch 1 - Save Best Score: 0.4653 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0691(0.1104) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 18m 26s) Loss: 0.1137(0.1137) Grad: inf  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 21s (remain 0m 0s) Loss: 0.1917(0.1185) Grad: 145997.0469  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 4m 24s) Loss: 0.1842(0.1842) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1185  avg_val_loss: 0.1065  time: 347s\n",
      "Epoch 2 - Score: 0.4571  Scores: [0.4900594368081922, 0.4886430384484807, 0.4138129900401851, 0.4386609368232969, 0.4733903215264121, 0.43831477840012784]\n",
      "Epoch 2 - Save Best Score: 0.4571 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0592(0.1065) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 18m 45s) Loss: 0.0436(0.0436) Grad: inf  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.0677(0.0904) Grad: 74417.7266  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 12s) Loss: 0.1835(0.1835) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0904  avg_val_loss: 0.1018  time: 349s\n",
      "Epoch 3 - Score: 0.4470  Scores: [0.4859789942342716, 0.44701407293805107, 0.4097602553633659, 0.4372144063174024, 0.4652053132200701, 0.4365596429513926]\n",
      "Epoch 3 - Save Best Score: 0.4470 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0549(0.1018) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== CV ==========\n",
      "Score: 0.4528  Scores: [0.4840649774001258, 0.4452264090550391, 0.41502430451923333, 0.4532989146930708, 0.4748279976780669, 0.44416351060842435]\n",
      "\u001b[32m[I 2022-12-14 07:03:13,734]\u001b[0m Trial 3 finished with value: 0.45276768565899334 and parameters: {'epoch': 4, 'encoder_lr': 0.0001859353113930986, 'llrd': 0.8181481143144653}. Best is trial 1 with value: 0.45065841781573773.\u001b[0m\n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111111111111111111111111111111\n",
      "0.45276768565899334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 20m 11s) Loss: 2.2423(2.2423) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 22s (remain 0m 0s) Loss: 0.0694(0.1807) Grad: 173533.2188  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 46s) Loss: 0.0935(0.0935) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1807  avg_val_loss: 0.1429  time: 349s\n",
      "Epoch 1 - Score: 0.5324  Scores: [0.5321247753009442, 0.5515747751116109, 0.47677813047794215, 0.5068260059868107, 0.5659841809732384, 0.5610722742877414]\n",
      "Epoch 1 - Save Best Score: 0.5324 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 26s (remain 0m 0s) Loss: 0.1782(0.1429) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 46s) Loss: 0.1303(0.1303) Grad: 470590.9375  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 22s (remain 0m 0s) Loss: 0.0473(0.1190) Grad: 141066.3125  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 46s) Loss: 0.0926(0.0926) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1190  avg_val_loss: 0.1069  time: 350s\n",
      "Epoch 2 - Score: 0.4584  Scores: [0.48677357049931236, 0.48798784694280667, 0.41524726414668856, 0.4511990428827202, 0.4685757263450551, 0.4405414645875082]\n",
      "Epoch 2 - Save Best Score: 0.4584 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1657(0.1069) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 19m 19s) Loss: 0.1606(0.1606) Grad: inf  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 22s (remain 0m 0s) Loss: 0.0870(0.0903) Grad: 179241.7812  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 45s) Loss: 0.0937(0.0937) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0903  avg_val_loss: 0.1020  time: 350s\n",
      "Epoch 3 - Score: 0.4476  Scores: [0.47572438237099846, 0.4419172479463272, 0.4099416774081043, 0.4527095299872726, 0.46523919665776464, 0.4400595538543649]\n",
      "Epoch 3 - Save Best Score: 0.4476 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1163(0.1020) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2934] Elapsed 0m 0s (remain 21m 4s) Loss: 3.4532(3.4532) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2933/2934] Elapsed 5m 23s (remain 0m 0s) Loss: 0.1311(0.1774) Grad: 239236.0625  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 4m 51s) Loss: 0.0908(0.0908) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1774  avg_val_loss: 0.1261  time: 351s\n",
      "Epoch 1 - Score: 0.4967  Scores: [0.5602818340555407, 0.4857470700336868, 0.45638557908492977, 0.4882685612022331, 0.519498948105762, 0.4700438185257302]\n",
      "Epoch 1 - Save Best Score: 0.4967 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0229(0.1261) \n",
      "Epoch: [2][0/2934] Elapsed 0m 0s (remain 19m 46s) Loss: 0.0789(0.0789) Grad: 381856.7500  LR: 0.00003753  \n",
      "Epoch: [2][2933/2934] Elapsed 5m 24s (remain 0m 0s) Loss: 0.0388(0.1198) Grad: 119289.4375  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 49s) Loss: 0.0689(0.0689) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1198  avg_val_loss: 0.1145  time: 352s\n",
      "Epoch 2 - Score: 0.4814  Scores: [0.49362594177740166, 0.46399764457203735, 0.5001400929937603, 0.5050946908073586, 0.46730221226495594, 0.45840879986134175]\n",
      "Epoch 2 - Save Best Score: 0.4814 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0707(0.1145) \n",
      "Epoch: [3][0/2934] Elapsed 0m 0s (remain 19m 59s) Loss: 0.0812(0.0812) Grad: 371468.1250  LR: 0.00001251  \n",
      "Epoch: [3][2933/2934] Elapsed 5m 23s (remain 0m 0s) Loss: 0.0567(0.0897) Grad: 163224.9219  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 44s) Loss: 0.0805(0.0805) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0897  avg_val_loss: 0.1050  time: 351s\n",
      "Epoch 3 - Score: 0.4546  Scores: [0.4894816380945003, 0.44154445898206074, 0.41959452442196454, 0.45570747384955856, 0.46996180653870884, 0.45137372738145826]\n",
      "Epoch 3 - Save Best Score: 0.4546 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0379(0.1050) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 22m 30s) Loss: 4.1793(4.1793) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.1165(0.1741) Grad: 454082.5000  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 33s) Loss: 0.1944(0.1944) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1741  avg_val_loss: 0.1206  time: 348s\n",
      "Epoch 1 - Score: 0.4859  Scores: [0.5339033001925289, 0.4677371918810751, 0.42359135475025655, 0.47328680523030275, 0.5189797418127945, 0.49789493410278574]\n",
      "Epoch 1 - Save Best Score: 0.4859 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0590(0.1206) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 20m 12s) Loss: 0.0879(0.0879) Grad: 395980.3750  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.0879(0.1128) Grad: 371777.5000  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 39s) Loss: 0.2208(0.2208) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1128  avg_val_loss: 0.1198  time: 348s\n",
      "Epoch 2 - Score: 0.4827  Scores: [0.525863480488479, 0.46337426626250666, 0.4284900413153502, 0.4797375604885301, 0.5377336219096756, 0.4611920915672263]\n",
      "Epoch 2 - Save Best Score: 0.4827 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.1225(0.1198) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 19m 32s) Loss: 0.0473(0.0473) Grad: 285675.5312  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 22s (remain 0m 0s) Loss: 0.0491(0.0907) Grad: 304970.6562  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 42s) Loss: 0.1814(0.1814) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0907  avg_val_loss: 0.1069  time: 347s\n",
      "Epoch 3 - Score: 0.4587  Scores: [0.48296855698014196, 0.44980454116810964, 0.41476715173322104, 0.4662655157530499, 0.4833470983269082, 0.45521327029603403]\n",
      "Epoch 3 - Save Best Score: 0.4587 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0660(0.1069) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 21m 17s) Loss: 3.6580(3.6580) Grad: 1763535.8750  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0930(0.1823) Grad: 204454.3906  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 13s) Loss: 0.1450(0.1450) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1823  avg_val_loss: 0.1302  time: 346s\n",
      "Epoch 1 - Score: 0.5099  Scores: [0.5008121311267075, 0.5139014613396031, 0.5677200406783475, 0.44328386187701235, 0.5897727235616838, 0.4439572705183704]\n",
      "Epoch 1 - Save Best Score: 0.5099 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0698(0.1302) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 43s) Loss: 0.2603(0.2603) Grad: inf  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0997(0.1183) Grad: 200279.4844  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 16s) Loss: 0.1471(0.1471) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1183  avg_val_loss: 0.1166  time: 346s\n",
      "Epoch 2 - Score: 0.4751  Scores: [0.5389268307869152, 0.4544218738576506, 0.4147343035966772, 0.49743549065885156, 0.507528073631663, 0.4376808750430226]\n",
      "Epoch 2 - Save Best Score: 0.4751 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.1004(0.1166) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 18m 48s) Loss: 0.1974(0.1974) Grad: inf  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 21s (remain 0m 0s) Loss: 0.0877(0.0915) Grad: 181081.5469  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 10s) Loss: 0.1845(0.1845) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0915  avg_val_loss: 0.1006  time: 347s\n",
      "Epoch 3 - Score: 0.4442  Scores: [0.4836364168301551, 0.4436298914281619, 0.4097932589011007, 0.4352860896095163, 0.4630394879603115, 0.4296404763018155]\n",
      "Epoch 3 - Save Best Score: 0.4442 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0541(0.1006) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== CV ==========\n",
      "Score: 0.4513  Scores: [0.4829757508698728, 0.4442371010674738, 0.41354228720598013, 0.452628399325356, 0.47046309010579, 0.44418299395322347]\n",
      "\u001b[32m[I 2022-12-14 08:13:34,315]\u001b[0m Trial 4 finished with value: 0.45133827042128266 and parameters: {'epoch': 3, 'encoder_lr': 0.00021056197663927353, 'llrd': 0.8946433972125326}. Best is trial 1 with value: 0.45065841781573773.\u001b[0m\n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111111111111111111111111111111\n",
      "0.45133827042128266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 20m 17s) Loss: 2.8933(2.8933) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 19s (remain 0m 0s) Loss: 0.0993(0.1773) Grad: 213936.7656  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 42s) Loss: 0.1035(0.1035) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1773  avg_val_loss: 0.1159  time: 347s\n",
      "Epoch 1 - Score: 0.4751  Scores: [0.49948726316095465, 0.4893178184868957, 0.41206102191823096, 0.4600399320193221, 0.5203573036978474, 0.4694634377403163]\n",
      "Epoch 1 - Save Best Score: 0.4751 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0750(0.1159) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 20m 12s) Loss: 0.1030(0.1030) Grad: 387766.6250  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0136(0.1151) Grad: 75311.3906  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 45s) Loss: 0.0782(0.0782) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1151  avg_val_loss: 0.1116  time: 348s\n",
      "Epoch 2 - Score: 0.4659  Scores: [0.47968199188687877, 0.45204782935364124, 0.41042923142941595, 0.4925500567553454, 0.5162524971772287, 0.4445142608129674]\n",
      "Epoch 2 - Save Best Score: 0.4659 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1615(0.1116) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 19m 21s) Loss: 0.0459(0.0459) Grad: 267137.7188  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0470(0.0898) Grad: 129346.3203  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 52s) Loss: 0.0775(0.0775) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0898  avg_val_loss: 0.1020  time: 347s\n",
      "Epoch 3 - Score: 0.4474  Scores: [0.4780496863622425, 0.4390593176636923, 0.410635235889175, 0.4519461087983157, 0.46571751251093585, 0.43923472525949625]\n",
      "Epoch 3 - Save Best Score: 0.4474 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1188(0.1020) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2934] Elapsed 0m 0s (remain 25m 22s) Loss: 3.4094(3.4094) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2933/2934] Elapsed 5m 19s (remain 0m 0s) Loss: 0.1156(0.1788) Grad: 195609.5312  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 4m 38s) Loss: 0.1255(0.1255) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1788  avg_val_loss: 0.1524  time: 347s\n",
      "Epoch 1 - Score: 0.5310  Scores: [0.7407462551052834, 0.4814167481871011, 0.4429926225620006, 0.46949816655679416, 0.5481210453003261, 0.5034620774457406]\n",
      "Epoch 1 - Save Best Score: 0.5310 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0632(0.1524) \n",
      "Epoch: [2][0/2934] Elapsed 0m 0s (remain 20m 23s) Loss: 0.1303(0.1303) Grad: inf  LR: 0.00003753  \n",
      "Epoch: [2][2933/2934] Elapsed 5m 20s (remain 0m 0s) Loss: 0.2377(0.1175) Grad: 313069.5312  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 36s) Loss: 0.0892(0.0892) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1175  avg_val_loss: 0.1166  time: 347s\n",
      "Epoch 2 - Score: 0.4815  Scores: [0.49059553248892196, 0.44673186512969504, 0.4668317382248286, 0.5213062438492861, 0.5079522807942597, 0.4556962675214505]\n",
      "Epoch 2 - Save Best Score: 0.4815 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0745(0.1166) \n",
      "Epoch: [3][0/2934] Elapsed 0m 0s (remain 20m 5s) Loss: 0.0881(0.0881) Grad: 372104.3750  LR: 0.00001251  \n",
      "Epoch: [3][2933/2934] Elapsed 5m 19s (remain 0m 0s) Loss: 0.0569(0.0906) Grad: 148342.0469  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 38s) Loss: 0.0895(0.0895) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0906  avg_val_loss: 0.1042  time: 347s\n",
      "Epoch 3 - Score: 0.4527  Scores: [0.4913532292871371, 0.44053618070168904, 0.41880659632774964, 0.451373564957113, 0.4667549431825424, 0.44755909316983483]\n",
      "Epoch 3 - Save Best Score: 0.4527 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0444(0.1042) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 20m 10s) Loss: 2.3385(2.3385) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 21s (remain 0m 0s) Loss: 0.0500(0.1730) Grad: 148966.6875  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 46s) Loss: 0.4039(0.4039) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1730  avg_val_loss: 0.1738  time: 346s\n",
      "Epoch 1 - Score: 0.5875  Scores: [0.709289189880479, 0.5030538816457579, 0.5877875363931852, 0.5509975483541382, 0.55850915200385, 0.6152128493373634]\n",
      "Epoch 1 - Save Best Score: 0.5875 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.4092(0.1738) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 20m 12s) Loss: 0.2322(0.2322) Grad: inf  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 22s (remain 0m 0s) Loss: 0.0722(0.1137) Grad: 175085.9062  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 36s) Loss: 0.1957(0.1957) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1137  avg_val_loss: 0.1081  time: 347s\n",
      "Epoch 2 - Score: 0.4648  Scores: [0.4838869118529056, 0.4510574891552395, 0.45948694756208724, 0.46837691169838586, 0.4742297605191585, 0.4516995109573594]\n",
      "Epoch 2 - Save Best Score: 0.4648 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.1068(0.1081) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 19m 59s) Loss: 0.0923(0.0923) Grad: 374714.2500  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 21s (remain 0m 0s) Loss: 0.0483(0.0888) Grad: 140327.5469  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 45s) Loss: 0.2132(0.2132) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0888  avg_val_loss: 0.1066  time: 346s\n",
      "Epoch 3 - Score: 0.4579  Scores: [0.48095925414402074, 0.44895790819779424, 0.41642501413612387, 0.46539953713825005, 0.4841813401603877, 0.4517483716488569]\n",
      "Epoch 3 - Save Best Score: 0.4579 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0779(0.1066) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 18m 45s) Loss: 3.2275(3.2275) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 19s (remain 0m 0s) Loss: 0.0698(0.1708) Grad: 332490.2188  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 8s) Loss: 0.3414(0.3414) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1708  avg_val_loss: 0.1445  time: 345s\n",
      "Epoch 1 - Score: 0.5291  Scores: [0.5495151528514929, 0.7510931355745231, 0.43247165916447494, 0.4906714600716729, 0.46642974399895715, 0.4844975228376682]\n",
      "Epoch 1 - Save Best Score: 0.5291 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0473(0.1445) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 50s) Loss: 0.0559(0.0559) Grad: 309358.5000  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0545(0.1154) Grad: 312996.9688  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 12s) Loss: 0.2435(0.2435) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1154  avg_val_loss: 0.1066  time: 346s\n",
      "Epoch 2 - Score: 0.4574  Scores: [0.502876582580452, 0.44365097394425984, 0.41523936347914736, 0.45188063108837456, 0.4652430082504392, 0.4655865710388337]\n",
      "Epoch 2 - Save Best Score: 0.4574 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0658(0.1066) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 19m 0s) Loss: 0.1087(0.1087) Grad: 420485.5625  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0434(0.0929) Grad: 302936.3438  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 18s) Loss: 0.1857(0.1857) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0929  avg_val_loss: 0.0998  time: 346s\n",
      "Epoch 3 - Score: 0.4426  Scores: [0.48040270623135267, 0.44356232142022395, 0.40566510867429306, 0.43690202901514724, 0.45878594684529156, 0.4303368282226238]\n",
      "Epoch 3 - Save Best Score: 0.4426 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0623(0.0998) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== CV ==========\n",
      "Score: 0.4502  Scores: [0.4827161330636967, 0.4430457692382024, 0.41291319395053366, 0.45151789160805145, 0.46895394164441534, 0.4422945491157398]\n",
      "\u001b[32m[I 2022-12-14 09:23:27,585]\u001b[0m Trial 5 finished with value: 0.45024024643677324 and parameters: {'epoch': 3, 'encoder_lr': 3.007918911639952e-05, 'llrd': 0.8329610635006021}. Best is trial 5 with value: 0.45024024643677324.\u001b[0m\n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111111111111111111111111111111\n",
      "0.45024024643677324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 19m 42s) Loss: 3.0329(3.0329) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 19s (remain 0m 0s) Loss: 0.1411(0.1845) Grad: 238735.0625  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 43s) Loss: 0.1042(0.1042) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1845  avg_val_loss: 0.1461  time: 346s\n",
      "Epoch 1 - Score: 0.5370  Scores: [0.5139165383173847, 0.4788407301557295, 0.522828784494016, 0.7696697438327399, 0.49374618096427025, 0.4430235120110605]\n",
      "Epoch 1 - Save Best Score: 0.5370 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1538(0.1461) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 20m 16s) Loss: 0.1008(0.1008) Grad: 449103.6250  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 17s (remain 0m 0s) Loss: 0.0467(0.1187) Grad: 133882.7812  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 52s) Loss: 0.0845(0.0845) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1187  avg_val_loss: 0.1059  time: 344s\n",
      "Epoch 2 - Score: 0.4563  Scores: [0.48381886888021086, 0.4548722585003184, 0.4203152215446149, 0.46591046862337077, 0.47411893018242623, 0.43889882700512756]\n",
      "Epoch 2 - Save Best Score: 0.4563 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 26s (remain 0m 0s) Loss: 0.1393(0.1059) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 18m 47s) Loss: 0.1734(0.1734) Grad: inf  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 19s (remain 0m 0s) Loss: 0.0581(0.0910) Grad: 146521.7344  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 51s) Loss: 0.0797(0.0797) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0910  avg_val_loss: 0.1023  time: 346s\n",
      "Epoch 3 - Score: 0.4483  Scores: [0.4780990380544392, 0.44213198660602454, 0.40963605193022884, 0.45661408177293145, 0.464141749103897, 0.43917488611553773]\n",
      "Epoch 3 - Save Best Score: 0.4483 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 26s (remain 0m 0s) Loss: 0.1281(0.1023) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2934] Elapsed 0m 0s (remain 20m 12s) Loss: 2.7532(2.7532) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2933/2934] Elapsed 5m 19s (remain 0m 0s) Loss: 0.0807(0.1750) Grad: 193144.5156  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 4m 50s) Loss: 0.1016(0.1016) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1750  avg_val_loss: 0.1219  time: 347s\n",
      "Epoch 1 - Score: 0.4862  Scores: [0.49370619024589496, 0.4736693441058998, 0.4397178363082871, 0.4546538094473245, 0.5783572990075301, 0.4773105413045735]\n",
      "Epoch 1 - Save Best Score: 0.4862 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0345(0.1219) \n",
      "Epoch: [2][0/2934] Elapsed 0m 0s (remain 19m 35s) Loss: 0.4023(0.4023) Grad: inf  LR: 0.00003753  \n",
      "Epoch: [2][2933/2934] Elapsed 5m 18s (remain 0m 0s) Loss: 0.1340(0.1149) Grad: 226164.6250  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 40s) Loss: 0.1326(0.1326) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1149  avg_val_loss: 0.1182  time: 346s\n",
      "Epoch 2 - Score: 0.4790  Scores: [0.5346811673123614, 0.4529020738568246, 0.42277199928274223, 0.45889854588353696, 0.5163922837590598, 0.4881954989182572]\n",
      "Epoch 2 - Save Best Score: 0.4790 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0411(0.1182) \n",
      "Epoch: [3][0/2934] Elapsed 0m 0s (remain 21m 20s) Loss: 0.0476(0.0476) Grad: 262911.1562  LR: 0.00001251  \n",
      "Epoch: [3][2933/2934] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0853(0.0892) Grad: 186389.2031  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 46s) Loss: 0.0759(0.0759) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0892  avg_val_loss: 0.1032  time: 348s\n",
      "Epoch 3 - Score: 0.4506  Scores: [0.4830526425653089, 0.44119883027202655, 0.4175186009553713, 0.44925697563633016, 0.4679162673723383, 0.4449050459338728]\n",
      "Epoch 3 - Save Best Score: 0.4506 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0374(0.1032) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 19m 51s) Loss: 3.1147(3.1147) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0426(0.1746) Grad: 126278.9219  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 39s) Loss: 0.2997(0.2997) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1746  avg_val_loss: 0.1384  time: 345s\n",
      "Epoch 1 - Score: 0.5231  Scores: [0.6060418966945579, 0.4756822919422532, 0.49872920459132886, 0.5683201806650514, 0.49535856762703434, 0.4943783175473378]\n",
      "Epoch 1 - Save Best Score: 0.5231 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.2805(0.1384) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 21s) Loss: 0.0968(0.0968) Grad: 379773.9062  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0382(0.1129) Grad: 123043.5547  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 38s) Loss: 0.1721(0.1721) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1129  avg_val_loss: 0.1136  time: 345s\n",
      "Epoch 2 - Score: 0.4748  Scores: [0.4888514046116735, 0.4593363391959792, 0.4527538209958638, 0.4701288604441095, 0.5074030480200942, 0.47042142425842015]\n",
      "Epoch 2 - Save Best Score: 0.4748 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0877(0.1136) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 19m 7s) Loss: 0.0361(0.0361) Grad: 227393.2969  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 21s (remain 0m 0s) Loss: 0.1415(0.0881) Grad: 251803.8906  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 39s) Loss: 0.1939(0.1939) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0881  avg_val_loss: 0.1071  time: 347s\n",
      "Epoch 3 - Score: 0.4590  Scores: [0.4820820765779571, 0.44994186531098473, 0.417744781564979, 0.4649935318825981, 0.4876864041930345, 0.4517005121883835]\n",
      "Epoch 3 - Save Best Score: 0.4590 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0588(0.1071) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 20m 52s) Loss: 2.9316(2.9316) Grad: 1728637.7500  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 21s (remain 0m 0s) Loss: 0.0350(0.1812) Grad: 127223.2344  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 6s) Loss: 0.2227(0.2227) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1812  avg_val_loss: 0.1156  time: 347s\n",
      "Epoch 1 - Score: 0.4730  Scores: [0.5600533446892185, 0.4613790518467088, 0.4211237020823644, 0.4503993094925236, 0.48781898564339304, 0.45714834045473646]\n",
      "Epoch 1 - Save Best Score: 0.4730 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0720(0.1156) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 14s) Loss: 0.1144(0.1144) Grad: inf  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.0462(0.1200) Grad: 130134.3203  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 8s) Loss: 0.1849(0.1849) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1200  avg_val_loss: 0.1154  time: 348s\n",
      "Epoch 2 - Score: 0.4725  Scores: [0.5482359877593409, 0.4458798281416888, 0.414557203264818, 0.5014676048060924, 0.4892761166106943, 0.43531576759218565]\n",
      "Epoch 2 - Save Best Score: 0.4725 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0644(0.1154) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 18m 14s) Loss: 0.3064(0.3064) Grad: inf  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 21s (remain 0m 0s) Loss: 0.2436(0.0919) Grad: 313826.5938  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 7s) Loss: 0.1839(0.1839) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0919  avg_val_loss: 0.1004  time: 347s\n",
      "Epoch 3 - Score: 0.4441  Scores: [0.4834708704547975, 0.44273810640797784, 0.41184560668629167, 0.43766901039089723, 0.4595603403764521, 0.42908810493148813]\n",
      "Epoch 3 - Save Best Score: 0.4441 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0547(0.1004) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== CV ==========\n",
      "Score: 0.4506  Scores: [0.4816804966161599, 0.4440169943932173, 0.4142004833515022, 0.45224555055693544, 0.46994916780702223, 0.44129405246185643]\n",
      "\u001b[32m[I 2022-12-14 10:33:18,926]\u001b[0m Trial 6 finished with value: 0.45056445753111557 and parameters: {'epoch': 4, 'encoder_lr': 0.0002259875438626923, 'llrd': 0.8349310270411133}. Best is trial 5 with value: 0.45024024643677324.\u001b[0m\n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111111111111111111111111111111\n",
      "0.45056445753111557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 24m 56s) Loss: 2.3857(2.3857) Grad: 7455847.5000  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 21s (remain 0m 0s) Loss: 0.1518(0.1767) Grad: 505189.7500  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 40s) Loss: 0.1126(0.1126) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1767  avg_val_loss: 0.1169  time: 349s\n",
      "Epoch 1 - Score: 0.4838  Scores: [0.48803070450807784, 0.5454331577411479, 0.46733538377081846, 0.4647515141747743, 0.47108069727842256, 0.46613838603225144]\n",
      "Epoch 1 - Save Best Score: 0.4838 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 26s (remain 0m 0s) Loss: 0.1147(0.1169) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 36s) Loss: 0.0493(0.0493) Grad: 277640.5938  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0488(0.1215) Grad: 136103.4375  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 39s) Loss: 0.0875(0.0875) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1215  avg_val_loss: 0.1118  time: 348s\n",
      "Epoch 2 - Score: 0.4687  Scores: [0.5056891926689322, 0.45831217509009886, 0.44143239956593994, 0.4581931572918767, 0.49714254115898143, 0.451324963922285]\n",
      "Epoch 2 - Save Best Score: 0.4687 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1851(0.1118) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 20m 16s) Loss: 0.0426(0.0426) Grad: 258435.4531  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0304(0.0915) Grad: 93213.8516  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 38s) Loss: 0.0801(0.0801) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0915  avg_val_loss: 0.1031  time: 348s\n",
      "Epoch 3 - Score: 0.4498  Scores: [0.478569821255421, 0.4437032652753412, 0.41320757404215763, 0.45643643032963116, 0.46895908445622564, 0.4380499701074223]\n",
      "Epoch 3 - Save Best Score: 0.4498 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 26s (remain 0m 0s) Loss: 0.1361(0.1031) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2934] Elapsed 0m 0s (remain 21m 46s) Loss: 4.0935(4.0935) Grad: 1703175.8750  LR: 0.00000625  \n",
      "Epoch: [1][2933/2934] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0723(0.1769) Grad: 166702.5469  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 5m 43s) Loss: 0.0423(0.0423) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1769  avg_val_loss: 0.1243  time: 348s\n",
      "Epoch 1 - Score: 0.4920  Scores: [0.5698605253085111, 0.44632146733011, 0.43702309358716784, 0.46869019838004583, 0.4797343527840959, 0.5504799136125166]\n",
      "Epoch 1 - Save Best Score: 0.4920 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1143(0.1243) \n",
      "Epoch: [2][0/2934] Elapsed 0m 0s (remain 19m 57s) Loss: 0.1369(0.1369) Grad: inf  LR: 0.00003753  \n",
      "Epoch: [2][2933/2934] Elapsed 5m 20s (remain 0m 0s) Loss: 0.2600(0.1123) Grad: 343927.6875  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 40s) Loss: 0.0946(0.0946) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1123  avg_val_loss: 0.1058  time: 348s\n",
      "Epoch 2 - Score: 0.4564  Scores: [0.4884121366447059, 0.451597453676487, 0.4198180945692375, 0.45348086175645724, 0.46967650866198873, 0.4553607170910343]\n",
      "Epoch 2 - Save Best Score: 0.4564 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0492(0.1058) \n",
      "Epoch: [3][0/2934] Elapsed 0m 0s (remain 20m 47s) Loss: 0.0514(0.0514) Grad: 284776.6875  LR: 0.00001251  \n",
      "Epoch: [3][2933/2934] Elapsed 5m 21s (remain 0m 0s) Loss: 0.0718(0.0893) Grad: 184784.7031  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 43s) Loss: 0.0819(0.0819) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0893  avg_val_loss: 0.1048  time: 348s\n",
      "Epoch 3 - Score: 0.4541  Scores: [0.4883614145178865, 0.4422303411675017, 0.4198413387896622, 0.4526747399088859, 0.47032723404744814, 0.45143539211811706]\n",
      "Epoch 3 - Save Best Score: 0.4541 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0417(0.1048) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 21m 14s) Loss: 2.0723(2.0723) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.1538(0.1824) Grad: 247629.7969  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 33s) Loss: 0.2367(0.2367) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1824  avg_val_loss: 0.1329  time: 348s\n",
      "Epoch 1 - Score: 0.5236  Scores: [0.5157741767541592, 0.5433350975335417, 0.5896734673475126, 0.4894401124604954, 0.4925721578051583, 0.5105823954496267]\n",
      "Epoch 1 - Save Best Score: 0.5236 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.1854(0.1329) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 52s) Loss: 0.1119(0.1119) Grad: 381611.2500  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.1777(0.1167) Grad: 307351.4375  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 36s) Loss: 0.1576(0.1576) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1167  avg_val_loss: 0.1157  time: 348s\n",
      "Epoch 2 - Score: 0.4777  Scores: [0.511900192063541, 0.46902927020699, 0.42099753824775143, 0.4872595047521829, 0.47951178254696925, 0.49738550021249184]\n",
      "Epoch 2 - Save Best Score: 0.4777 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0554(0.1157) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 24m 26s) Loss: 0.1111(0.1111) Grad: 407140.3438  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.0246(0.0877) Grad: 90521.5156  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 41s) Loss: 0.2009(0.2009) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0877  avg_val_loss: 0.1073  time: 348s\n",
      "Epoch 3 - Score: 0.4596  Scores: [0.48059826111245446, 0.4498540921495614, 0.41789178365111485, 0.46586526428794384, 0.4872550712097543, 0.45606954458992494]\n",
      "Epoch 3 - Save Best Score: 0.4596 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0768(0.1073) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 19m 54s) Loss: 1.9190(1.9190) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 21s (remain 0m 0s) Loss: 0.1375(0.1811) Grad: 247409.4375  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 14s) Loss: 0.2611(0.2611) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1811  avg_val_loss: 0.1177  time: 347s\n",
      "Epoch 1 - Score: 0.4806  Scores: [0.5063534182970807, 0.49195190843056114, 0.42297986886932404, 0.4459388146805447, 0.49866228937208995, 0.5174834560433352]\n",
      "Epoch 1 - Save Best Score: 0.4806 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0784(0.1177) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 20m 8s) Loss: 0.1144(0.1144) Grad: 425961.7188  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0124(0.1139) Grad: 57407.2227  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 14s) Loss: 0.1807(0.1807) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1139  avg_val_loss: 0.1135  time: 346s\n",
      "Epoch 2 - Score: 0.4707  Scores: [0.5615651638336483, 0.44944729333435307, 0.443732418926062, 0.4695169766298437, 0.4680821143393613, 0.4317341268199231]\n",
      "Epoch 2 - Save Best Score: 0.4707 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0819(0.1135) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 19m 55s) Loss: 0.0951(0.0951) Grad: 412739.0312  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 21s (remain 0m 0s) Loss: 0.0389(0.0904) Grad: 127670.5000  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 15s) Loss: 0.1893(0.1893) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0904  avg_val_loss: 0.1010  time: 347s\n",
      "Epoch 3 - Score: 0.4454  Scores: [0.48562962666166687, 0.4424161564443774, 0.4107116429931964, 0.4384357339666918, 0.4620633432688018, 0.4329820571657352]\n",
      "Epoch 3 - Save Best Score: 0.4454 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0711(0.1010) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== CV ==========\n",
      "Score: 0.4523  Scores: [0.48330418563479793, 0.4445624637222645, 0.4154278144660297, 0.4534605000421865, 0.47224257896511, 0.44473262725260043]\n",
      "\u001b[32m[I 2022-12-14 11:43:25,698]\u001b[0m Trial 7 finished with value: 0.45228836168049824 and parameters: {'epoch': 3, 'encoder_lr': 0.0004596564335419012, 'llrd': 0.9410059335933649}. Best is trial 5 with value: 0.45024024643677324.\u001b[0m\n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111111111111111111111111111111\n",
      "0.45228836168049824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 20m 36s) Loss: 3.6337(3.6337) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0328(0.1775) Grad: 233481.4844  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 39s) Loss: 0.1302(0.1302) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1775  avg_val_loss: 0.1278  time: 348s\n",
      "Epoch 1 - Score: 0.4996  Scores: [0.5473628005548506, 0.4529038173654368, 0.4330159621496183, 0.5413836913693465, 0.505828775706767, 0.5168767995616693]\n",
      "Epoch 1 - Save Best Score: 0.4996 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 26s (remain 0m 0s) Loss: 0.1844(0.1278) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 18m 50s) Loss: 0.2715(0.2715) Grad: 698529.8750  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0540(0.1148) Grad: 282233.5312  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 38s) Loss: 0.0948(0.0948) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1148  avg_val_loss: 0.1105  time: 348s\n",
      "Epoch 2 - Score: 0.4616  Scores: [0.5371740378289024, 0.4418641705711978, 0.4068432780544594, 0.460046277652631, 0.4815428712443458, 0.44189722990911323]\n",
      "Epoch 2 - Save Best Score: 0.4616 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0972(0.1105) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 20m 8s) Loss: 0.0201(0.0201) Grad: 171478.6406  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0809(0.0939) Grad: 394164.0938  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 43s) Loss: 0.0788(0.0788) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0939  avg_val_loss: 0.1025  time: 347s\n",
      "Epoch 3 - Score: 0.4483  Scores: [0.47983628579898235, 0.44243498750739335, 0.4075380621727886, 0.45411947843842715, 0.4670218259685753, 0.4388466961932903]\n",
      "Epoch 3 - Save Best Score: 0.4483 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 26s (remain 0m 0s) Loss: 0.1202(0.1025) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2934] Elapsed 0m 0s (remain 18m 52s) Loss: 2.8276(2.8276) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2933/2934] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0489(0.1743) Grad: 132347.4531  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 50s) Loss: 0.0475(0.0475) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1743  avg_val_loss: 0.1281  time: 348s\n",
      "Epoch 1 - Score: 0.5006  Scores: [0.524734275818752, 0.45431344813518776, 0.4334246190144043, 0.5319306052180206, 0.5334931838781944, 0.5255997781127707]\n",
      "Epoch 1 - Save Best Score: 0.5006 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0746(0.1281) \n",
      "Epoch: [2][0/2934] Elapsed 0m 0s (remain 19m 51s) Loss: 0.0951(0.0951) Grad: 384102.6875  LR: 0.00003753  \n",
      "Epoch: [2][2933/2934] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0441(0.1177) Grad: 125283.3359  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 48s) Loss: 0.0952(0.0952) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1177  avg_val_loss: 0.1149  time: 348s\n",
      "Epoch 2 - Score: 0.4735  Scores: [0.5053046058957499, 0.4446725112266623, 0.42243973453173866, 0.45874705653834325, 0.513531578616305, 0.4961816724136345]\n",
      "Epoch 2 - Save Best Score: 0.4735 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0562(0.1149) \n",
      "Epoch: [3][0/2934] Elapsed 0m 0s (remain 19m 2s) Loss: 0.0546(0.0546) Grad: 307735.5938  LR: 0.00001251  \n",
      "Epoch: [3][2933/2934] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0499(0.0901) Grad: 149514.1094  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 38s) Loss: 0.0805(0.0805) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0901  avg_val_loss: 0.1049  time: 348s\n",
      "Epoch 3 - Score: 0.4542  Scores: [0.49115892767611125, 0.44414963965087084, 0.4201513743427517, 0.450739918891948, 0.4702892549132214, 0.44871069826572024]\n",
      "Epoch 3 - Save Best Score: 0.4542 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0485(0.1049) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 19m 53s) Loss: 2.7283(2.7283) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 22s (remain 0m 0s) Loss: 0.2604(0.1798) Grad: 328051.5625  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 37s) Loss: 0.1088(0.1088) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1798  avg_val_loss: 0.2433  time: 347s\n",
      "Epoch 1 - Score: 0.6921  Scores: [0.6340075037387738, 0.5675727831017525, 0.8442164832306411, 0.6415061278854826, 0.9905295640382908, 0.47468586810924807]\n",
      "Epoch 1 - Save Best Score: 0.6921 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.1114(0.2433) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 18m 35s) Loss: 0.5781(0.5781) Grad: inf  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 22s (remain 0m 0s) Loss: 0.0796(0.1187) Grad: 173605.3750  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 41s) Loss: 0.2252(0.2252) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1187  avg_val_loss: 0.1155  time: 347s\n",
      "Epoch 2 - Score: 0.4802  Scores: [0.4911824361694115, 0.45287855554223877, 0.4647526898607107, 0.4673148932488088, 0.4907008303518718, 0.5144393586008082]\n",
      "Epoch 2 - Save Best Score: 0.4802 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.1039(0.1155) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 19m 44s) Loss: 0.1799(0.1799) Grad: inf  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.1473(0.0904) Grad: 251000.3750  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 40s) Loss: 0.1855(0.1855) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0904  avg_val_loss: 0.1071  time: 345s\n",
      "Epoch 3 - Score: 0.4591  Scores: [0.4798415173422024, 0.45129601038791217, 0.41540452850764736, 0.4667405974945159, 0.4875595150531324, 0.4534842352116871]\n",
      "Epoch 3 - Save Best Score: 0.4591 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0724(0.1071) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 21m 29s) Loss: 2.5515(2.5515) Grad: 1659668.6250  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 22s (remain 0m 0s) Loss: 0.0864(0.1815) Grad: 181157.3281  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 11s) Loss: 0.3137(0.3137) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1815  avg_val_loss: 0.1249  time: 348s\n",
      "Epoch 1 - Score: 0.4909  Scores: [0.5856330362953542, 0.46795679756554814, 0.42621628434589853, 0.5049949532442929, 0.5036456341128028, 0.45695722287777624]\n",
      "Epoch 1 - Save Best Score: 0.4909 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0333(0.1249) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 53s) Loss: 0.0373(0.0373) Grad: 233497.4062  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 22s (remain 0m 0s) Loss: 0.0429(0.1199) Grad: 123940.4844  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 13s) Loss: 0.2052(0.2052) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1199  avg_val_loss: 0.1034  time: 348s\n",
      "Epoch 2 - Score: 0.4503  Scores: [0.4912487590471541, 0.4616500134513257, 0.4136843843133541, 0.4379425957762976, 0.46617816907277976, 0.43138151978259603]\n",
      "Epoch 2 - Save Best Score: 0.4503 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0464(0.1034) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 19m 40s) Loss: 0.0361(0.0361) Grad: 239963.5156  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 21s (remain 0m 0s) Loss: 0.0315(0.0898) Grad: 105502.0000  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 10s) Loss: 0.1734(0.1734) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0898  avg_val_loss: 0.1013  time: 347s\n",
      "Epoch 3 - Score: 0.4459  Scores: [0.48492124927266317, 0.44284406755052763, 0.4102225618427285, 0.43820976340258605, 0.46440545143156636, 0.4345969502109852]\n",
      "Epoch 3 - Save Best Score: 0.4459 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0608(0.1013) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== CV ==========\n",
      "Score: 0.4519  Scores: [0.48396003850637304, 0.44519589203527365, 0.41335581841558844, 0.45256691348673395, 0.4724061054036974, 0.44397236660578077]\n",
      "\u001b[32m[I 2022-12-14 12:53:26,438]\u001b[0m Trial 8 finished with value: 0.45190952240890786 and parameters: {'epoch': 3, 'encoder_lr': 9.285261289172484e-05, 'llrd': 0.9641014940094981}. Best is trial 5 with value: 0.45024024643677324.\u001b[0m\n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111111111111111111111111111111\n",
      "0.45190952240890786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 19m 43s) Loss: 1.9886(1.9886) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 21s (remain 0m 0s) Loss: 0.1455(0.1758) Grad: 242756.0781  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 47s) Loss: 0.0892(0.0892) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1758  avg_val_loss: 0.1104  time: 349s\n",
      "Epoch 1 - Score: 0.4636  Scores: [0.4925511473589047, 0.44970703358796615, 0.4144324444678159, 0.4579226387466673, 0.5085390288753759, 0.4585310193141039]\n",
      "Epoch 1 - Save Best Score: 0.4636 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1214(0.1104) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 58s) Loss: 0.0360(0.0360) Grad: 242302.7969  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 21s (remain 0m 0s) Loss: 0.0250(0.1148) Grad: 97176.7031  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 53s) Loss: 0.0727(0.0727) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1148  avg_val_loss: 0.1071  time: 349s\n",
      "Epoch 2 - Score: 0.4573  Scores: [0.4806559703323133, 0.4420633136219639, 0.4084533447119033, 0.4891894760718548, 0.49039806976279793, 0.43275236976157366]\n",
      "Epoch 2 - Save Best Score: 0.4573 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1097(0.1071) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 20m 49s) Loss: 0.1954(0.1954) Grad: 577561.4375  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.0374(0.0894) Grad: 114010.3750  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 47s) Loss: 0.0806(0.0806) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0894  avg_val_loss: 0.1030  time: 350s\n",
      "Epoch 3 - Score: 0.4495  Scores: [0.48158808179896523, 0.44304819833666576, 0.4117665658570596, 0.45406636458330574, 0.47011193257892225, 0.43661001285914597]\n",
      "Epoch 3 - Save Best Score: 0.4495 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1073(0.1030) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2934] Elapsed 0m 0s (remain 21m 22s) Loss: 2.8493(2.8493) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2933/2934] Elapsed 5m 22s (remain 0m 0s) Loss: 0.1361(0.1780) Grad: 222052.3125  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 46s) Loss: 0.1360(0.1360) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1780  avg_val_loss: 0.1256  time: 350s\n",
      "Epoch 1 - Score: 0.4971  Scores: [0.5009151934521958, 0.4487419339040933, 0.423511027460066, 0.5944004342771521, 0.4984237720662074, 0.5165034192770426]\n",
      "Epoch 1 - Save Best Score: 0.4971 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0764(0.1256) \n",
      "Epoch: [2][0/2934] Elapsed 0m 0s (remain 20m 43s) Loss: 0.0515(0.0515) Grad: 300675.0625  LR: 0.00003753  \n",
      "Epoch: [2][2933/2934] Elapsed 5m 21s (remain 0m 0s) Loss: 0.0728(0.1134) Grad: 173330.9219  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 33s) Loss: 0.0677(0.0677) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1134  avg_val_loss: 0.1218  time: 348s\n",
      "Epoch 2 - Score: 0.4944  Scores: [0.505482332797298, 0.5331656263074394, 0.48128295987482944, 0.4610733819828682, 0.4821892685663741, 0.5031317790511235]\n",
      "Epoch 2 - Save Best Score: 0.4944 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.1040(0.1218) \n",
      "Epoch: [3][0/2934] Elapsed 0m 0s (remain 19m 43s) Loss: 0.1177(0.1177) Grad: 437673.3438  LR: 0.00001251  \n",
      "Epoch: [3][2933/2934] Elapsed 5m 21s (remain 0m 0s) Loss: 0.1083(0.0900) Grad: 211001.0469  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 43s) Loss: 0.0738(0.0738) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0900  avg_val_loss: 0.1044  time: 349s\n",
      "Epoch 3 - Score: 0.4532  Scores: [0.48870800954486926, 0.44075948310594687, 0.41996461772940485, 0.45237295931231625, 0.4698932064961157, 0.4475213099038892]\n",
      "Epoch 3 - Save Best Score: 0.4532 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0453(0.1044) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 20m 41s) Loss: 1.1914(1.1914) Grad: 1813047.7500  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 24s (remain 0m 0s) Loss: 0.0764(0.1785) Grad: 181447.7969  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 37s) Loss: 0.3730(0.3730) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1785  avg_val_loss: 0.1550  time: 349s\n",
      "Epoch 1 - Score: 0.5484  Scores: [0.6953253926155508, 0.5015849868012198, 0.5249772960032247, 0.47633663289286693, 0.5385063745436284, 0.5533866351517914]\n",
      "Epoch 1 - Save Best Score: 0.5484 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.2819(0.1550) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 45s) Loss: 0.0360(0.0360) Grad: 220014.8125  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 24s (remain 0m 0s) Loss: 0.1379(0.1154) Grad: 230074.5312  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 33s) Loss: 0.2656(0.2656) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1154  avg_val_loss: 0.1362  time: 350s\n",
      "Epoch 2 - Score: 0.5235  Scores: [0.5550163717850229, 0.4558705039382968, 0.5361513650711325, 0.5036747116816747, 0.5055509048804121, 0.5846053923454266]\n",
      "Epoch 2 - Save Best Score: 0.5235 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.2021(0.1362) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 19m 30s) Loss: 0.2488(0.2488) Grad: inf  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.0483(0.0891) Grad: 144386.2188  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 2m 39s) Loss: 0.1703(0.1703) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0891  avg_val_loss: 0.1072  time: 348s\n",
      "Epoch 3 - Score: 0.4594  Scores: [0.480207408619614, 0.4489605502226418, 0.41818598109799604, 0.4672863379572216, 0.4877316522775543, 0.45406055951722823]\n",
      "Epoch 3 - Save Best Score: 0.4594 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 24s (remain 0m 0s) Loss: 0.0738(0.1072) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "attention.0.weight\n",
      "attention.0.bias\n",
      "attention.1.weight\n",
      "attention.1.bias\n",
      "attention.3.weight\n",
      "attention.3.bias\n",
      "Epoch: [1][0/2933] Elapsed 0m 0s (remain 22m 40s) Loss: 2.2425(2.2425) Grad: inf  LR: 0.00000625  \n",
      "Epoch: [1][2932/2933] Elapsed 5m 23s (remain 0m 0s) Loss: 0.0610(0.1752) Grad: 347151.7812  LR: 0.00003754  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 9s) Loss: 0.1199(0.1199) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1752  avg_val_loss: 0.1494  time: 349s\n",
      "Epoch 1 - Score: 0.5321  Scores: [0.564431939326412, 0.4609599197940835, 0.46049733954475175, 0.46273008477991306, 0.6744385217093577, 0.5696571863057972]\n",
      "Epoch 1 - Save Best Score: 0.5321 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.1092(0.1494) \n",
      "Epoch: [2][0/2933] Elapsed 0m 0s (remain 19m 51s) Loss: 0.0973(0.0973) Grad: 375117.2812  LR: 0.00003753  \n",
      "Epoch: [2][2932/2933] Elapsed 5m 20s (remain 0m 0s) Loss: 0.0469(0.1155) Grad: 264605.5625  LR: 0.00001252  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 3s) Loss: 0.1888(0.1888) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1155  avg_val_loss: 0.1034  time: 346s\n",
      "Epoch 2 - Score: 0.4524  Scores: [0.48823698667384946, 0.4432369930720677, 0.4358960499990727, 0.44452162421593433, 0.4628015290709445, 0.4398998397312368]\n",
      "Epoch 2 - Save Best Score: 0.4524 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0473(0.1034) \n",
      "Epoch: [3][0/2933] Elapsed 0m 0s (remain 20m 30s) Loss: 0.3832(0.3832) Grad: 806098.5000  LR: 0.00001251  \n",
      "Epoch: [3][2932/2933] Elapsed 5m 22s (remain 0m 0s) Loss: 0.0731(0.0918) Grad: 180559.8594  LR: 0.00000000  \n",
      "EVAL: [0/489] Elapsed 0m 0s (remain 3m 9s) Loss: 0.1630(0.1630) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0918  avg_val_loss: 0.1004  time: 348s\n",
      "Epoch 3 - Score: 0.4440  Scores: [0.4807670287807327, 0.4434742693413729, 0.40963217531780977, 0.43559801052061237, 0.464886314218015, 0.42945756491663684]\n",
      "Epoch 3 - Save Best Score: 0.4440 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [488/489] Elapsed 0m 25s (remain 0m 0s) Loss: 0.0546(0.1004) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== CV ==========\n",
      "Score: 0.4516  Scores: [0.4828283467175599, 0.44407167999176844, 0.41490833398215304, 0.45247093741560873, 0.4732360743123661, 0.44201340710333087]\n",
      "\u001b[32m[I 2022-12-14 14:03:43,234]\u001b[0m Trial 9 finished with value: 0.4515881299204645 and parameters: {'epoch': 4, 'encoder_lr': 0.000134831353541911, 'llrd': 0.9103563486857054}. Best is trial 5 with value: 0.45024024643677324.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111111111111111111111111111111\n",
      "0.4515881299204645\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TrialState' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38990/3732503236.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mpruned_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRUNED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mcomplete_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TrialState' is not defined"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "    \n",
    "def get_result(oof_df):\n",
    "        labels = oof_df[CFG.target_cols].values\n",
    "        preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
    "        score, scores = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
    "        return score\n",
    "def objective(trial):\n",
    "\n",
    "    CFG.print_freq = np.inf\n",
    "    # CFG.num_warmup_step = trial.suggest_int('num_warmup_step',\n",
    "    CFG.epoch = trial.suggest_int('epoch',3,4)\n",
    "    encoder_lr = trial.suggest_float('encoder_lr',5e-6,5e-4)\n",
    "    decoder_lr = 2*encoder_lr\n",
    "    llrd = trial.suggest_float('llrd',0.8,0.99)\n",
    "\n",
    "    oof_df = pd.DataFrame()\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold in CFG.trn_fold:\n",
    "            _oof_df = train_loop(train, fold)\n",
    "            oof_df = pd.concat([oof_df, _oof_df])\n",
    "            # LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "            \n",
    "    oof_df = oof_df.reset_index(drop=True)\n",
    "    print(11111111111111111111111111111111)\n",
    "    LOGGER.info(f\"========== CV ==========\")\n",
    "    score = get_result(oof_df)\n",
    "    print(score)\n",
    "    return score\n",
    "        \n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, timeout=40000)\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False)\n",
    "    complete_trials = study.get_trials(deepcopy=False)\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fc55c1e-d16d-4654-be53-7d84bea1ca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  10\n",
      "  Number of pruned trials:  10\n",
      "  Number of complete trials:  10\n",
      "Best trial:\n",
      "  Value:  0.45024024643677324\n",
      "  Params: \n",
      "    epoch: 3\n",
      "    encoder_lr: 3.007918911639952e-05\n",
      "    llrd: 0.8329610635006021\n"
     ]
    }
   ],
   "source": [
    "pruned_trials = study.get_trials(deepcopy=False)\n",
    "complete_trials = study.get_trials(deepcopy=False)\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61ceca-7eee-4021-ba60-aab6fd78320c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
