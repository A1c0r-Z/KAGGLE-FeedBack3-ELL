# KAGGLE-FeedBack3-ELL
This is a conclusion of Feedback Prize - English Language Learning [here](https://www.kaggle.com/competitions/feedback-prize-english-language-learning).\
During the competition, I got 129/2654 on public lb,but got only 591/2654 on private lb.So I want to do this competition again and make a late submission.
## train
During the competition I only used deberta-v3-base and deberta-v3-large with mean pooling and attention pooling.\
This time I'm going to train more model
### model used:
1.deberta-v3-base
2.deberta-v3-large
3.deberta-v2-xlarge
4.roberta-large
5.distilbert-base-uncased
